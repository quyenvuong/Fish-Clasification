{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import csv\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Flatten, Dense, SimpleRNN, LSTM, Dropout, BatchNormalization, GlobalAveragePooling1D, Bidirectional, GRU\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = []\n",
    "labels = []\n",
    "stopwords = [ \"a\", \"about\", \"above\", \"after\", \"again\", \"against\", \"all\", \"am\", \"an\", \"and\", \"any\", \"are\", \"as\", \"at\", \"be\", \"because\", \"been\", \"before\", \"being\", \"below\", \"between\", \"both\", \"but\", \"by\", \"could\", \"did\", \"do\", \"does\", \"doing\", \"down\", \"during\", \"each\", \"few\", \"for\", \"from\", \"further\", \"had\", \"has\", \"have\", \"having\", \"he\", \"he'd\", \"he'll\", \"he's\", \"her\", \"here\", \"here's\", \"hers\", \"herself\", \"him\", \"himself\", \"his\", \"how\", \"how's\", \"i\", \"i'd\", \"i'll\", \"i'm\", \"i've\", \"if\", \"in\", \"into\", \"is\", \"it\", \"it's\", \"its\", \"itself\", \"let's\", \"me\", \"more\", \"most\", \"my\", \"myself\", \"nor\", \"of\", \"on\", \"once\", \"only\", \"or\", \"other\", \"ought\", \"our\", \"ours\", \"ourselves\", \"out\", \"over\", \"own\", \"same\", \"she\", \"she'd\", \"she'll\", \"she's\", \"should\", \"so\", \"some\", \"such\", \"than\", \"that\", \"that's\", \"the\", \"their\", \"theirs\", \"them\", \"themselves\", \"then\", \"there\", \"there's\", \"these\", \"they\", \"they'd\", \"they'll\", \"they're\", \"they've\", \"this\", \"those\", \"through\", \"to\", \"too\", \"under\", \"until\", \"up\", \"very\", \"was\", \"we\", \"we'd\", \"we'll\", \"we're\", \"we've\", \"were\", \"what\", \"what's\", \"when\", \"when's\", \"where\", \"where's\", \"which\", \"while\", \"who\", \"who's\", \"whom\", \"why\", \"why's\", \"with\", \"would\", \"you\", \"you'd\", \"you'll\", \"you're\", \"you've\", \"your\", \"yours\", \"yourself\", \"yourselves\" ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file = open('Corona_NLP_train.csv', mode='r', encoding= 'ISO-8859-1')\n",
    "train = csv.DictReader(csv_file)\n",
    "for row in train:\n",
    "    sentence = row['OriginalTweet']\n",
    "    for word in stopwords:\n",
    "        token = \" \" + word + \" \"\n",
    "        sentence = sentence.replace(token, \" \")\n",
    "    sentence = re.sub(r'http\\S+', '', sentence)\n",
    "    sentence = re.sub(\"[^a-zA-Z0-9]\",\" \",sentence)\n",
    "    #sentence = re.sub(r'@\\w+','',sentence)\n",
    "    #sentence = re.sub(r'\\w+:\\/{2}[\\d\\w-]+(\\.[\\d\\w-]+)*(?:(?:\\/[^\\s/]*))*', '', sentence)\n",
    "    text.append(sentence)\n",
    "#     if 'Positive' in row['Sentiment']:\n",
    "#         labels.append('Positive')\n",
    "#     elif 'Negative' in row['Sentiment']:\n",
    "#         labels.append('Negative')\n",
    "#     else:\n",
    "#         labels.append('Neutral')\n",
    "    labels.append(row['Sentiment'])\n",
    "labels = list(map(lambda x: x.replace(' ',''), labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen = 100 #max(map(len, text)) # 344\n",
    "train_sample = int(0.8*len(labels))\n",
    "validation = len(labels) - train_sample\n",
    "max_words = 10000\n",
    "embedding = 16\n",
    "trunc_type = 'post'\n",
    "padding_type = 'post'\n",
    "oov_tok = '<OOV>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=max_words, oov_token=oov_tok)\n",
    "tokenizer.fit_on_texts(text)\n",
    "sequences = tokenizer.texts_to_sequences(text)\n",
    "word_index = tokenizer.word_index\n",
    "print('found %s unique tokens.' %len(word_index) + 1)\n",
    "\n",
    "data = pad_sequences(sequences, maxlen=maxlen, padding=padding_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_tokenizer = Tokenizer()\n",
    "label_tokenizer.fit_on_texts(labels)\n",
    "label_seq = np.asarray(label_tokenizer.texts_to_sequences(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_seq.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = np.arange(data.shape[0])\n",
    "np.random.seed(2)\n",
    "np.random.shuffle(indices)\n",
    "data = data[indices]\n",
    "label_seq = label_seq[indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = data[:train_sample]\n",
    "y_train = label_seq[:train_sample]\n",
    "x_val = data[train_sample: train_sample + validation]\n",
    "y_val = label_seq[train_sample: train_sample + validation]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#glove_dir = 'D:/Desktop_D/Python Code For Deep Learning With Python/Chapter 6/glove.6B'\n",
    "#embedding_index = {}\n",
    "#f = open(os.path.join(glove_dir, 'glove.6B.100d.txt'), encoding = 'utf-8')\n",
    "#for line in f:\n",
    "#    values = line.split()\n",
    "#    word = values[0]\n",
    "#    coefs = np.asarray(values[1:], dtype='float32')\n",
    "#    embedding_index[word] = coefs\n",
    "#f.close()\n",
    "#print('Found %s word vectors' % len(embedding_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#embedding = 100\n",
    "\n",
    "#embedding_matrix = np.zeros((max_words, embedding))\n",
    "#for word, i in word_index.items():\n",
    "#    if i < max_words:\n",
    "#        embedding_vector = embedding_index.get(word)\n",
    "#        if embedding_vector is not None:\n",
    "#           embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = Sequential()\n",
    "#model.add(Embedding(max_words, embedding, input_length=maxlen))\n",
    "#model.add(Flatten())\n",
    "#model.add(Embedding(max_words, embedding))\n",
    "#model.add(LSTM(32))\n",
    "#model.add(SimpleRNN(32, return_sequences = True))\n",
    "#model.add(SimpleRNN(32))\n",
    "#model.add(Dense(16, activation='relu'))\n",
    "#model.add(Dense(8, activation='relu'))\n",
    "#model.add(Dense(6, activation='softmax'))\n",
    "#model.add(Dropout(0.5))\n",
    "#model.add(Dense(4, activation='softmax'))\n",
    "\n",
    "#model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 200\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_words, embedding, input_length=maxlen))\n",
    "model.add(LSTM(128, return_sequences = True))\n",
    "model.add(LSTM(128, return_sequences = True))\n",
    "model.add(LSTM(128, return_sequences = True))\n",
    "model.add(BatchNormalization())\n",
    "model.add(GlobalAveragePooling1D())\n",
    "model.add(Dense(64, activation = 'relu'))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(5, activation='softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.layers[0].set_weights([embedding_matrix])\n",
    "#model.layers[0].trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class myCallback(tf.keras.callbacks.Callback):\n",
    "  def on_test_batch_end(self, epoch, logs={}):\n",
    "    if(logs.get('acc')>0.85):\n",
    "      print(\"\\nReached 85% accuracy so cancelling training!\")\n",
    "      self.model.stop_training = True\n",
    "\n",
    "callbacks = myCallback()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics='acc')\n",
    "\n",
    "history = model.fit(x_train,\n",
    "                    y_train,\n",
    "                    epochs=10,\n",
    "                    batch_size=200,\n",
    "                    validation_data=(x_val, y_val),\n",
    "                    callbacks = [callbacks])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def plot_graphs(history, string):\n",
    "    plt.plot(history.history[string])\n",
    "    plt.plot(history.history['val_' + string])\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(string)\n",
    "    plt.legend([string, 'val_' + string])\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_graphs(history, \"acc\")\n",
    "plot_graphs(history, \"loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_file = open('Corona_NLP_test.csv', mode = 'r', encoding = 'utf-8')\n",
    "test = csv.DictReader(test_file, delimiter = ',')\n",
    "test_sentences = []\n",
    "test_labels = []\n",
    "for row in test:\n",
    "    test_sentences.append(row['OriginalTweet'])\n",
    "#     if 'Positive' in row['Sentiment']:\n",
    "#         test_labels.append('Positive')\n",
    "#     elif 'Negative' in row['Sentiment']:\n",
    "#         test_labels.append('Negative')\n",
    "#     else:\n",
    "#         test_labels.append('Neutral')\n",
    "   test_labels.append(row['Sentiment'])\n",
    "test_labels = list(map(lambda x: x.replace(' ',''), test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sequences = tokenizer.texts_to_sequences(test_sentences)\n",
    "test_data = pad_sequences(test_sequences, maxlen=maxlen, padding=padding_type)\n",
    "test_label_seq = np.asarray(label_tokenizer.texts_to_sequences(test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(test_data, test_label_seq)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
